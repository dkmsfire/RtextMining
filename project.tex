\documentclass[a4pper,11pt,onecolumn]{article}
\setlength\parindent{2em}
\usepackage{graphicx}
\usepackage{mathtools}
\title{Web Searching and Mining Project }
\author{Shu Hong Liu}
\begin{document}
\maketitle
%abstract
\begin{abstract}
	abstract\\
{\bf Keywords:} TF-IDF, Okapi, Langauage Model
\end{abstract}
%section
\section{Introduction}

I used WT2G datasets which is part of WT10G from TREC Web Corpus.
Since 1992, a series of annual benchmarking evaluation exercises , called TREC (Text REtrieval Conference), have launched in the USA. TREC experiments were designed to allow large-scale laboratory testing, compare the effectiveness and performance of different information retrieval techniques. TREC has become the standard in the IR field. So, here I used WT2G that is a smaller datasets from TREC.

In WT2G, we can find that there are 247491 documents and about 1,500,000 unique words. That is a big challenge for me, because I studied in Statistic and the number of documents is too big for me to do something. Fortunately, professor Tsai recommend Indri toolkit to do information retrieval more easily than I thought before.
\section{Method}
Here I am going to introduce the methods that I used.
\subsection{Vector Space Model}
Okapi Tf and IDF
\subsection{Language model with Laplace smoothing}

\subsection{Language model with Jelinek-Mercer smoothing}
In Jelinek-Mercer smoothing, I take
\section{Trials}
Before using methods that I metioned above, I construct two indexs, (1) without stemming, and (2) with stemming. In each index, I used all of methods above, so there will be 2 * 4 runs.
\subsection{without stemming}
\subsubsection{TF-IDF}
\subsubsection{Okapi}
\subsection{with stemming}

\section{Idea}
In many trials , We can see that the results are bad enough. However , that means there's still room for improvement
\section{Conclusion}
Maybe in the next time, we do this project that we can use more powerful method.
\paragraph{paragraph}
 test paragraph
%indent
\\\indent test indent
\end{document}
